{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k0PLfkF9ah8J",
    "outputId": "dea9aa89-5f15-4e7a-fb07-bbcd04887b07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Drive directory: /content/drive/MyDrive/BigMart\n"
     ]
    }
   ],
   "source": [
    "# === Colab setup\n",
    "!pip install --quiet xgboost lightgbm catboost optuna shap plotly scikit-learn optuna-integration[lightgbm] optuna-integration[xgboost] optuna-integration[sklearn]\n",
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "import os\n",
    "DRIVE_DIR = '/content/drive/MyDrive/BigMart'\n",
    "if not os.path.exists(DRIVE_DIR):\n",
    "    os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "print('Drive directory:', DRIVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQyR_bjda1N5",
    "outputId": "090e35b9-cdc5-4b37-e3c2-0b73ccf46d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BigMart Sales Prediction - Fixed Pipeline\n",
      "================================================================================\n",
      "\n",
      "Loading data...\n",
      "Train shape: (8523, 12), Test shape: (5681, 11)\n",
      "Starting preprocessing...\n",
      "Preprocessing complete. Shape: (8523, 65)\n",
      "Preprocessing test data...\n",
      "Test preprocessing complete. Shape: (5681, 61)\n",
      "\n",
      "Final shapes - Train: (8523, 63), Test: (5681, 63)\n",
      "\n",
      "================================================================================\n",
      "Training LightGBM with GroupKFold\n",
      "================================================================================\n",
      "\n",
      "Fold 1/5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[85]\ttraining's rmse: 1009.33\tvalid_1's rmse: 1992.87\n",
      "Fold 1 RMSE: 1992.8652\n",
      "\n",
      "Fold 2/5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's rmse: 1200.28\tvalid_1's rmse: 1499.05\n",
      "Fold 2 RMSE: 1499.0546\n",
      "\n",
      "Fold 3/5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's rmse: 1068.74\tvalid_1's rmse: 1125.61\n",
      "Fold 3 RMSE: 1125.6107\n",
      "\n",
      "Fold 4/5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's rmse: 1065.19\tvalid_1's rmse: 1070.74\n",
      "Fold 4 RMSE: 1070.7409\n",
      "\n",
      "Fold 5/5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\ttraining's rmse: 1488.16\tvalid_1's rmse: 1502.25\n",
      "Fold 5 RMSE: 1502.2541\n",
      "\n",
      "LightGBM CV RMSE: 1438.1051 (+/- 331.1187)\n",
      "\n",
      "================================================================================\n",
      "Training XGBoost with GroupKFold\n",
      "================================================================================\n",
      "\n",
      "Fold 1/5\n",
      "[0]\ttrain-rmse:1524.67153\teval-rmse:2368.07431\n",
      "[500]\ttrain-rmse:1029.84137\teval-rmse:1777.79524\n",
      "[1000]\ttrain-rmse:1007.82106\teval-rmse:1767.11664\n",
      "[1500]\ttrain-rmse:989.14590\teval-rmse:1764.50469\n",
      "[1972]\ttrain-rmse:974.13209\teval-rmse:1765.23256\n",
      "Fold 1 RMSE: 1765.2460\n",
      "\n",
      "Fold 2/5\n",
      "[0]\ttrain-rmse:1700.62188\teval-rmse:1713.12367\n",
      "[500]\ttrain-rmse:1084.40110\teval-rmse:1507.88638\n",
      "[503]\ttrain-rmse:1084.23330\teval-rmse:1507.85826\n",
      "Fold 2 RMSE: 1508.0549\n",
      "\n",
      "Fold 3/5\n",
      "[0]\ttrain-rmse:1755.71512\teval-rmse:1470.60355\n",
      "[473]\ttrain-rmse:1035.34091\teval-rmse:1199.99017\n",
      "Fold 3 RMSE: 1199.9902\n",
      "\n",
      "Fold 4/5\n",
      "[0]\ttrain-rmse:1760.29941\teval-rmse:1449.82087\n",
      "[500]\ttrain-rmse:1050.85163\teval-rmse:1077.94196\n",
      "[611]\ttrain-rmse:1044.35299\teval-rmse:1079.49681\n",
      "Fold 4 RMSE: 1079.4968\n",
      "\n",
      "Fold 5/5\n",
      "[0]\ttrain-rmse:1735.01407\teval-rmse:1560.07746\n",
      "[500]\ttrain-rmse:1033.05501\teval-rmse:1345.52111\n",
      "[688]\ttrain-rmse:1022.26582\teval-rmse:1348.64512\n",
      "Fold 5 RMSE: 1348.5671\n",
      "\n",
      "XGBoost CV RMSE: 1380.2710 (+/- 240.2107)\n",
      "\n",
      "================================================================================\n",
      "Training Random Forest with GroupKFold\n",
      "================================================================================\n",
      "\n",
      "Fold 1/5\n",
      "Fold 1 RMSE: 1829.2330\n",
      "\n",
      "Fold 2/5\n",
      "Fold 2 RMSE: 1041.6187\n",
      "\n",
      "Fold 3/5\n",
      "Fold 3 RMSE: 1142.8402\n",
      "\n",
      "Fold 4/5\n",
      "Fold 4 RMSE: 1126.7889\n",
      "\n",
      "Fold 5/5\n",
      "Fold 5 RMSE: 1174.2843\n",
      "\n",
      "Random Forest CV RMSE: 1262.9530 (+/- 286.5281)\n",
      "\n",
      "================================================================================\n",
      "Training Meta-Model (Ridge)\n",
      "================================================================================\n",
      "Meta-Model RMSE: 1264.9673\n",
      "Weights: {'lgbm': np.float64(-0.046419914475870516), 'rf': np.float64(1.2078140723234416), 'xgb': np.float64(-0.03870530155430133)}\n",
      "\n",
      "Saved meta-model predictions: predictions_meta_stacked.csv\n",
      "\n",
      "================================================================================\n",
      "Pipeline completed successfully!\n",
      "predictions_meta_stacked.csv \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "BigMart Sales Prediction Pipeline\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, List\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import Ridge\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    INPUT_DIR = '/content/drive/MyDrive/BigMart'\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/BigMart/Output'\n",
    "    RANDOM_STATE = 3\n",
    "    N_SPLITS = 5\n",
    "    LGBM_ROUNDS = 20000\n",
    "    XGB_ROUNDS = 15000\n",
    "    EARLY_STOPPING_ROUNDS = 300\n",
    "    REFERENCE_YEAR = 2025\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Handles all feature engineering operations\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "        self.aggregated_features = {}\n",
    "\n",
    "    def create_item_category(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract item category from identifier\"\"\"\n",
    "        if 'Item_Category' not in df.columns:\n",
    "            df['Item_Category'] = df['Item_Identifier'].astype(str).str[:2]\n",
    "        return df\n",
    "\n",
    "    def fix_fat_content(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Standardize fat content labels\"\"\"\n",
    "        df['Item_Fat_Content'] = df['Item_Fat_Content'].replace({\n",
    "            'low fat': 'Low Fat', 'LF': 'Low Fat', 'reg': 'Regular'\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Impute missing values\"\"\"\n",
    "        item_weight_median = df.groupby('Item_Identifier')['Item_Weight'].transform('median')\n",
    "        global_weight_median = df['Item_Weight'].median()\n",
    "        df['Item_Weight'] = df['Item_Weight'].fillna(item_weight_median).fillna(global_weight_median)\n",
    "\n",
    "        df.loc[df['Item_Visibility'] == 0, 'Item_Visibility'] = np.nan\n",
    "        visibility_median = df.groupby('Item_Type')['Item_Visibility'].transform('median')\n",
    "        df['Item_Visibility'] = df['Item_Visibility'].fillna(visibility_median)\n",
    "\n",
    "        outlet_size_mode = df.groupby('Outlet_Type')['Outlet_Size'].transform(\n",
    "            lambda x: x.mode().iloc[0] if not x.mode().empty else \"Medium\"\n",
    "        )\n",
    "        df['Outlet_Size'] = df['Outlet_Size'].fillna(outlet_size_mode).fillna(\"Medium\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_basic_features(self, df: pd.DataFrame, reference_year: int) -> pd.DataFrame:\n",
    "        \"\"\"Create basic engineered features with fixed reference year\"\"\"\n",
    "        df['Outlet_Age'] = reference_year - df['Outlet_Establishment_Year']\n",
    "\n",
    "        # Outlet age bins\n",
    "        df['Outlet_Age_Bin'] = pd.cut(\n",
    "            df['Outlet_Age'],\n",
    "            bins=[0, 5, 10, 15, 100],\n",
    "            labels=['Very_New', 'New', 'Established', 'Old']\n",
    "        )\n",
    "\n",
    "        df['Item_Visibility_Bin'] = pd.qcut(\n",
    "            df['Item_Visibility'],\n",
    "            q=3,\n",
    "            labels=['Low_Vis', 'Medium_Vis', 'High_Vis'],\n",
    "            duplicates='drop'\n",
    "        )\n",
    "\n",
    "        df['Item_MRP_Bin'] = pd.cut(\n",
    "            df['Item_MRP'],\n",
    "            bins=[0, 70, 140, 200, 3000],\n",
    "            labels=['Low_Price', 'Medium_Price', 'High_Price', 'Premium_Price']\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_interaction_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create interaction features\"\"\"\n",
    "        df['MRP_Weight'] = df['Item_MRP'] * df['Item_Weight']\n",
    "        df['MRP_Visibility'] = df['Item_MRP'] * df['Item_Visibility']\n",
    "\n",
    "        df['Weight_Visibility'] = df['Item_Weight'] * df['Item_Visibility']\n",
    "\n",
    "        return df\n",
    "\n",
    "    def create_aggregated_features_train(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create and store aggregated features from training data only\"\"\"\n",
    "        # Outlet-level aggregations\n",
    "        outlet_aggs = df.groupby('Outlet_Identifier').agg({\n",
    "            'Item_MRP': ['mean', 'std', 'median'],\n",
    "            'Item_Weight': ['mean', 'std'],\n",
    "            'Item_Visibility': ['mean', 'std']\n",
    "        }).reset_index()\n",
    "        outlet_aggs.columns = ['Outlet_Identifier'] + [\n",
    "            f'Outlet_{col[0]}_{col[1]}' for col in outlet_aggs.columns[1:]\n",
    "        ]\n",
    "        self.aggregated_features['outlet'] = outlet_aggs\n",
    "        df = df.merge(outlet_aggs, on='Outlet_Identifier', how='left')\n",
    "\n",
    "\n",
    "        # Unique item ratio feature\n",
    "        outlet_item_counts = df.groupby('Outlet_Identifier').agg({\n",
    "            'Item_Identifier': ['nunique', 'count']\n",
    "        }).reset_index()\n",
    "        outlet_item_counts.columns = ['Outlet_Identifier', 'Unique_Items', 'Total_Items']\n",
    "        outlet_item_counts['Unique_Item_Ratio'] = outlet_item_counts['Unique_Items'] / outlet_item_counts['Total_Items']\n",
    "\n",
    "        df = df.merge(outlet_item_counts[['Outlet_Identifier', 'Unique_Item_Ratio', 'Unique_Items', 'Total_Items']],\n",
    "                      on='Outlet_Identifier', how='left')\n",
    "\n",
    "        self.aggregated_features['outlet_items'] = outlet_item_counts[['Outlet_Identifier', 'Unique_Item_Ratio', 'Unique_Items', 'Total_Items']]\n",
    "\n",
    "        # Item-type aggregations\n",
    "        item_type_aggs = df.groupby('Item_Type').agg({\n",
    "            'Item_MRP': ['mean', 'std'],\n",
    "            'Item_Visibility': ['mean', 'std'],\n",
    "            'Item_Weight': ['mean']\n",
    "        }).reset_index()\n",
    "        item_type_aggs.columns = ['Item_Type'] + [\n",
    "            f'ItemType_{col[0]}_{col[1]}' for col in item_type_aggs.columns[1:]\n",
    "        ]\n",
    "        self.aggregated_features['item_type'] = item_type_aggs\n",
    "        df = df.merge(item_type_aggs, on='Item_Type', how='left')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def apply_aggregated_features_test(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply stored aggregated features to test data\"\"\"\n",
    "        df = df.merge(self.aggregated_features['outlet'], on='Outlet_Identifier', how='left')\n",
    "        df = df.merge(self.aggregated_features['item_type'], on='Item_Type', how='left')\n",
    "        return df\n",
    "\n",
    "    def label_encode(self, df: pd.DataFrame, cols: List[str], fit: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Label encode categorical variables\"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        for col in cols:\n",
    "            if col in df.columns:\n",
    "                if fit:\n",
    "                    self.label_encoders[col] = LabelEncoder()\n",
    "                    df[col] = self.label_encoders[col].fit_transform(df[col].astype(str))\n",
    "                else:\n",
    "                    # Handle unseen categories\n",
    "                    df[col] = df[col].astype(str).map(\n",
    "                        lambda x: self.label_encoders[col].transform([x])[0]\n",
    "                        if x in self.label_encoders[col].classes_\n",
    "                        else -1\n",
    "                    )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def encode_categorical(self, df: pd.DataFrame, fit: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Encode categorical variables\"\"\"\n",
    "\n",
    "        label_encode_cols = ['Item_Identifier', 'Outlet_Identifier']\n",
    "        df = self.label_encode(df, label_encode_cols, fit=fit)\n",
    "\n",
    "        if 'Item_Fat_Content' in df.columns:\n",
    "            df['Item_Fat_Content'] = df['Item_Fat_Content'].map({\n",
    "                'Low Fat': 0, 'Regular': 1, 'Non-Edible': 2\n",
    "            })\n",
    "\n",
    "        if 'Outlet_Size' in df.columns:\n",
    "            df['Outlet_Size'] = df['Outlet_Size'].map({\n",
    "                'Small': 0, 'Medium': 1, 'High': 2\n",
    "            })\n",
    "\n",
    "        categorical_cols = ['Item_Type', 'Outlet_Type', 'Outlet_Location_Type',\n",
    "                           'Outlet_Age_Bin', 'Item_Visibility_Bin',\n",
    "                           'Item_MRP_Bin', 'Item_Category']\n",
    "\n",
    "        existing_cols = [col for col in categorical_cols if col in df.columns]\n",
    "\n",
    "        df = pd.get_dummies(df, columns=existing_cols,\n",
    "                           prefix=existing_cols, drop_first=False)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESSING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "class Preprocessor:\n",
    "    \"\"\"Main preprocessing pipeline\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.feature_engineer = FeatureEngineer()\n",
    "\n",
    "    def preprocess_train(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Full preprocessing pipeline for training data\"\"\"\n",
    "        print(\"Starting preprocessing...\")\n",
    "        df = df.copy()\n",
    "\n",
    "        # Feature engineering\n",
    "        df = self.feature_engineer.create_item_category(df)\n",
    "        df = self.feature_engineer.fix_fat_content(df)\n",
    "        df = self.feature_engineer.handle_missing_values(df)\n",
    "        df = self.feature_engineer.create_basic_features(df, self.config.REFERENCE_YEAR)\n",
    "        df = self.feature_engineer.create_interaction_features(df)\n",
    "        df = self.feature_engineer.create_aggregated_features_train(df)  # Compute on train only\n",
    "        df = self.feature_engineer.encode_categorical(df, fit=True)\n",
    "\n",
    "        print(f\"Preprocessing complete. Shape: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "    def preprocess_test(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Preprocessing pipeline for test data\"\"\"\n",
    "        print(\"Preprocessing test data...\")\n",
    "        df = df.copy()\n",
    "\n",
    "        df = self.feature_engineer.create_item_category(df)\n",
    "        df = self.feature_engineer.fix_fat_content(df)\n",
    "        df = self.feature_engineer.handle_missing_values(df)\n",
    "        df = self.feature_engineer.create_basic_features(df, self.config.REFERENCE_YEAR)\n",
    "        df = self.feature_engineer.create_interaction_features(df)\n",
    "        df = self.feature_engineer.apply_aggregated_features_test(df)\n",
    "        df = self.feature_engineer.encode_categorical(df, fit=False)\n",
    "\n",
    "        print(f\"Test preprocessing complete. Shape: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Handles model training with GroupKFold cross-validation\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "\n",
    "    def train_lightgbm(self, X: pd.DataFrame, y: pd.Series, groups: pd.Series,\n",
    "                       X_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Train LightGBM with GroupKFold CV\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Training LightGBM with GroupKFold\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'num_leaves': 116,\n",
    "            'learning_rate': 0.09450119316286584,\n",
    "            'max_depth': 4,\n",
    "            'feature_fraction': 0.9320909184684667,\n",
    "            'bagging_fraction': 0.6797788057232733,\n",
    "            'bagging_freq': 5,\n",
    "            'min_child_samples': 60,\n",
    "            'min_child_weight': 0.0386883317530884,\n",
    "            'min_gain_to_split': 0.0549201598381078,\n",
    "            'reg_alpha': 9.477209577153645e-08,\n",
    "            'reg_lambda': 0.11698784340860989,\n",
    "            'verbose': -1\n",
    "        }\n",
    "\n",
    "        gkf = GroupKFold(n_splits=self.config.N_SPLITS)\n",
    "\n",
    "        oof_preds = np.zeros(len(X))\n",
    "        test_preds = np.zeros(len(X_test))\n",
    "        rmses = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups), 1):\n",
    "            print(f\"\\nFold {fold}/{self.config.N_SPLITS}\")\n",
    "\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_data,\n",
    "                num_boost_round=self.config.LGBM_ROUNDS,\n",
    "                valid_sets=[train_data, val_data],\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(stopping_rounds=self.config.EARLY_STOPPING_ROUNDS),\n",
    "                    lgb.log_evaluation(period=500)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            oof_preds[val_idx] = model.predict(X_val)\n",
    "            test_preds += model.predict(X_test) / self.config.N_SPLITS\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, oof_preds[val_idx]))\n",
    "            rmses.append(rmse)\n",
    "            print(f\"Fold {fold} RMSE: {rmse:.4f}\")\n",
    "\n",
    "        print(f\"\\nLightGBM CV RMSE: {np.mean(rmses):.4f} (+/- {np.std(rmses):.4f})\")\n",
    "        return oof_preds, test_preds\n",
    "\n",
    "    def train_xgboost(self, X: pd.DataFrame, y: pd.Series, groups: pd.Series,\n",
    "                      X_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Train XGBoost with GroupKFold CV\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Training XGBoost with GroupKFold\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        params = {\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "            'seed': self.config.RANDOM_STATE,\n",
    "            'verbosity': 0,\n",
    "            'tree_method': 'hist',\n",
    "            'max_depth': 3,\n",
    "            'learning_rate': 0.011789839446789884,\n",
    "            'subsample': 0.8195852322660473,\n",
    "            'colsample_bytree': 0.9380241079482217,\n",
    "            'colsample_bylevel': 0.5556572065430505,\n",
    "            'min_child_weight': 26,\n",
    "            'gamma': 4.265164162900841,\n",
    "            'reg_alpha': 0.35322223713188117,\n",
    "            'reg_lambda': 0.0028308377883415736\n",
    "        }\n",
    "\n",
    "        gkf = GroupKFold(n_splits=self.config.N_SPLITS)\n",
    "\n",
    "        oof_preds = np.zeros(len(X))\n",
    "        test_preds = np.zeros(len(X_test))\n",
    "        rmses = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups), 1):\n",
    "            print(f\"\\nFold {fold}/{self.config.N_SPLITS}\")\n",
    "\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "            model = xgb.train(\n",
    "                params,\n",
    "                dtrain,\n",
    "                num_boost_round=self.config.XGB_ROUNDS,\n",
    "                evals=[(dtrain, 'train'), (dval, 'eval')],\n",
    "                early_stopping_rounds=self.config.EARLY_STOPPING_ROUNDS,\n",
    "                verbose_eval=500\n",
    "            )\n",
    "\n",
    "            oof_preds[val_idx] = model.predict(dval)\n",
    "            test_preds += model.predict(xgb.DMatrix(X_test)) / self.config.N_SPLITS\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, oof_preds[val_idx]))\n",
    "            rmses.append(rmse)\n",
    "            print(f\"Fold {fold} RMSE: {rmse:.4f}\")\n",
    "\n",
    "        print(f\"\\nXGBoost CV RMSE: {np.mean(rmses):.4f} (+/- {np.std(rmses):.4f})\")\n",
    "        return oof_preds, test_preds\n",
    "\n",
    "    def train_random_forest(self, X: pd.DataFrame, y: pd.Series, groups: pd.Series,\n",
    "                           X_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Train Random Forest with GroupKFold CV\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Training Random Forest with GroupKFold\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        params = {\n",
    "            \"n_estimators\": 500,\n",
    "            \"max_depth\": 10,\n",
    "            \"min_samples_split\": 3,\n",
    "            \"min_samples_leaf\": 5,\n",
    "            \"max_features\": \"sqrt\",\n",
    "            \"random_state\": self.config.RANDOM_STATE,\n",
    "            \"n_jobs\": -1,\n",
    "            \"verbose\": 0\n",
    "        }\n",
    "\n",
    "        gkf = GroupKFold(n_splits=self.config.N_SPLITS)\n",
    "\n",
    "        oof_preds = np.zeros(len(X))\n",
    "        test_preds = np.zeros(len(X_test))\n",
    "        rmses = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups), 1):\n",
    "            print(f\"\\nFold {fold}/{self.config.N_SPLITS}\")\n",
    "\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            model = RandomForestRegressor(**params)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            oof_preds[val_idx] = model.predict(X_val)\n",
    "            test_preds += model.predict(X_test) / self.config.N_SPLITS\n",
    "\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, oof_preds[val_idx]))\n",
    "            rmses.append(rmse)\n",
    "            print(f\"Fold {fold} RMSE: {rmse:.4f}\")\n",
    "\n",
    "        print(f\"\\nRandom Forest CV RMSE: {np.mean(rmses):.4f} (+/- {np.std(rmses):.4f})\")\n",
    "        return oof_preds, test_preds\n",
    "\n",
    "    def train_meta_model(self, oof_preds_dict: dict, y_train: pd.Series,\n",
    "                        test_preds_dict: dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Train meta-model (Ridge) on out-of-fold predictions\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Training Meta-Model (Ridge)\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        X_meta = np.column_stack([oof_preds_dict[name] for name in sorted(oof_preds_dict.keys())])\n",
    "        X_test_meta = np.column_stack([test_preds_dict[name] for name in sorted(test_preds_dict.keys())])\n",
    "\n",
    "        meta_model = Ridge(alpha=10.0, random_state=self.config.RANDOM_STATE)\n",
    "        meta_model.fit(X_meta, y_train)\n",
    "\n",
    "        oof_meta = meta_model.predict(X_meta)\n",
    "        test_meta = meta_model.predict(X_test_meta)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_train, oof_meta))\n",
    "        print(f\"Meta-Model RMSE: {rmse:.4f}\")\n",
    "        print(f\"Weights: {dict(zip(sorted(oof_preds_dict.keys()), meta_model.coef_))}\")\n",
    "\n",
    "        return oof_meta, test_meta\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"BigMart Sales Prediction - Fixed Pipeline\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    config = Config()\n",
    "\n",
    "    # Load data\n",
    "    print(\"\\nLoading data...\")\n",
    "    train_df = pd.read_csv(os.path.join(config.INPUT_DIR, 'train.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(config.INPUT_DIR, 'test.csv'))\n",
    "    print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "\n",
    "    # Store test identifiers\n",
    "    test_ids = test_df[['Item_Identifier', 'Outlet_Identifier']].copy()\n",
    "\n",
    "    # Store outlet groups for GroupKFold BEFORE encoding\n",
    "    outlet_groups = train_df['Outlet_Identifier'].copy()\n",
    "\n",
    "    # Preprocess\n",
    "    preprocessor = Preprocessor(config)\n",
    "    train_processed = preprocessor.preprocess_train(train_df)\n",
    "    test_processed = preprocessor.preprocess_test(test_df)\n",
    "\n",
    "    # Prepare features\n",
    "    drop_cols = ['Item_Outlet_Sales', 'Outlet_Establishment_Year']\n",
    "    X_train = train_processed.drop(columns=drop_cols, errors='ignore')\n",
    "    y_train = train_processed['Item_Outlet_Sales']\n",
    "    X_test = test_processed.drop(columns=['Outlet_Establishment_Year'], errors='ignore')\n",
    "\n",
    "    # Align columns\n",
    "    missing_cols = set(X_train.columns) - set(X_test.columns)\n",
    "    for col in missing_cols:\n",
    "        X_test[col] = 0\n",
    "    X_test = X_test[X_train.columns]\n",
    "\n",
    "    print(f\"\\nFinal shapes - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    # Train models\n",
    "    trainer = ModelTrainer(config)\n",
    "    oof_lgbm, test_lgbm = trainer.train_lightgbm(X_train, y_train, outlet_groups, X_test)\n",
    "    oof_xgb, test_xgb = trainer.train_xgboost(X_train, y_train, outlet_groups, X_test)\n",
    "    oof_rf, test_rf = trainer.train_random_forest(X_train, y_train, outlet_groups, X_test)\n",
    "\n",
    "    # Ensemble\n",
    "    oof_preds = {'lgbm': oof_lgbm, 'xgb': oof_xgb, 'rf': oof_rf}\n",
    "    test_preds = {'lgbm': test_lgbm, 'xgb': test_xgb, 'rf': test_rf}\n",
    "    oof_meta, test_meta = trainer.train_meta_model(oof_preds, y_train, test_preds)\n",
    "\n",
    "    # Save\n",
    "    os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "    #Meta-model predictions (stacked ensemble)\n",
    "    output_meta = test_ids.copy()\n",
    "    output_meta['Item_Outlet_Sales'] = np.maximum(test_meta, 0)\n",
    "    output_meta.to_csv(os.path.join(config.OUTPUT_DIR, 'predictions_meta_stacked.csv'), index=False)\n",
    "    print(\"\\nSaved meta-model predictions: predictions_meta_stacked.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Pipeline completed successfully!\")\n",
    "    print(\"predictions_meta_stacked.csv \")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
